MOFormer 结构简介与实验流程梳理

一、MOFormer 模型结构概述

MOFormer 是专为金属有机框架（MOFs）属性预测设计的 Transformer 架构深度学习模型。其核心特点如下：

1. 输入处理（MOFid 分词与嵌入）
- 输入为 MOFid（MOF 的文本标识符），包含两部分：金属节点和有机连接体的 SMILES 字符串，以及拓扑编码（RCSR 数据库）。
- 分词：分别用定制分词器处理 SMILES 和拓扑，二者用 "&&" 连接，序列首尾加 [CLS] 和 [SEP]。
- 序列长度固定为 512，不足用 [PAD] 补齐，超长截断（仅 0.37% hMOF 数据需截断）。
- 嵌入：每个 token 转为 512 维向量，并加上正弦/余弦位置编码，保留位置信息。

2. Transformer 编码器
- 采用 6 层 Transformer，每层包括：
  - 多头自注意力（Multi-Head Self-Attention）：通过可学习权重生成 Query/Key/Value，计算缩放点积注意力（通常 8 或 12 头）。
  - 前馈网络（FFN）：两层线性变换+激活（如 GELU）。
  - 残差连接与层归一化，提升训练稳定性和收敛速度。

3. 输出与任务头
- 用 [CLS] 标记的输出嵌入（512 维）作为 MOFid 的全局表示，输入多层感知机（MLP）进行属性预测（如带隙、气体吸附量）。

4. 自监督预训练框架
- MOFormer 与晶体图卷积神经网络（CGCNN）联合自监督预训练，采用双模态输入（文本 MOFid + 结构 3D 晶体图）。
- 两模型分别生成嵌入，计算交叉相关矩阵，优化 Barlow Twins 损失，使两种表示对齐。
- 预训练数据量：413,535 个唯一 MOF（CoRE MOF 2019 + hMOF + Boyd & Woo）。

5. 设计优势
- 结构无关：仅需 MOFid 文本，无需 3D 结构，适合大规模虚拟筛选。
- 自监督预训练显著提升性能（带隙 MAE 降低 5.34%，气体吸附 MAE 降低 4.3%）。
- 注意力可解释性强，能聚焦关键原子和拓扑信息。
- 数据高效：小样本下对量子化学属性预测优于 CGCNN。

结构流程图（简化）：
MOFid (文本) → 分词 → 嵌入+位置编码 → 6 层 Transformer → [CLS] 向量 → MLP 任务头 → 属性预测

二、论文实验设计与流程

1. 数据集准备与预处理
- 预训练数据：合并 CoRE MOF 2019、hMOF、Boyd & Woo，去重后共 413,535 个 MOF。
- 下游任务数据：QMOF（带隙预测，1,119 个 MOF）、hMOF（CO₂/CH₄ 吸附量预测，137,652 个 MOF）。
- 所有数据需同时包含 MOFid 和 3D 结构。
- 预处理：MOFid 分词（定制分词器，最长 512），结构数据用于 CGCNN/SOAP。训练/测试集按 8:2 划分。

2. 模型对比实验
- 结构无关模型：MOFormer、Stoichiometric-120、RACs（仅需 MOFid/化学式）。
- 结构相关模型：CGCNN、SOAP（需 3D 结构）。
- 评估指标：MAE（主）、R²（辅）。
- 结果：MOFormer 在带隙预测上（MAE=0.46 eV）优于 SOAP（0.52 eV）等，且无需结构输入。气体吸附预测 CGCNN 最优，MOFormer 明显优于传统结构无关方法。

3. 自监督预训练（SSL）实验
- 框架：MOFormer（文本）+ CGCNN（结构），Barlow Twins 损失（λ=0.0051）。
- 效果：预训练后 MOFormer 带隙 MAE 降低 5.34%，气体吸附 MAE 降低 4.3%；CGCNN 气体吸附 MAE 降低 16.5%。

4. 可视化与可解释性
- t-SNE 降维：MOFormer 表示按拓扑聚类，CGCNN 按吸附量聚类。
- 注意力热力图：MOFormer 关注关键原子（如 Y、O）、拓扑编码（如 pcu）和有机连接体中的双键，验证模型可解释性。

5. 数据效率实验
- 任务：带隙预测、CO₂ 吸附预测。
- 变量：训练集大小（100–100,000）。
- 结果：小样本下 MOFormer > CGCNN（带隙预测）；气体吸附 CGCNN 始终最优。SOAP 小数据表现好但扩展性差。

6. 主要结论
- MOFormer 优势：结构无关、虚拟筛选快、量子化学属性预测优于结构相关模型、预训练提升 5–16%。
- CGCNN 优势：结构相关任务表现最优，预训练提升数据效率。
- 局限：MOFormer 对罕见拓扑预测较弱，SOAP 扩展性有限。

实验流程总结（流程图）：
数据集准备 → 模型训练 → 对比实验 → SSL 预训练 → 可视化分析 → 数据效率测试 → 结论

三、预训练轮次与训练细节

- 预训练轮次：100 个 Epoch，Batch Size 256，总迭代约 161,500 次。
- 优化器：AdamW，初始学习率 1e-4，线性衰减（Warmup + Cosine Decay）。
- 硬件：4 × NVIDIA V100 GPU 分布式训练。
- 依据：Supplementary Table S3 明确标注上述超参数。
- 选择 100 Epochs 原因：
  - 损失在 100 Epochs 后趋于稳定（收敛性验证）。
  - 训练时长约 40–50 小时，兼顾资源与效果。
  - 预训练后 MAE 显著下降，证明 100 Epochs 足够。
- 微调阶段：下游任务微调 200 Epochs，Batch Size 64–128，学习率 5e-5。
- 代码实现：开源代码 pretrain.py/finetune.py 默认参数与论文一致。

阶段对照表：

| 阶段         | 轮次 (Epochs) | Batch Size | 数据量         | 主要目标                 |
| ------------ | ------------- | ---------- | -------------- | ------------------------ |
| 自监督预训练 | 100           | 256        | 413,535 MOF    | 学习结构-文本表示对齐    |
| 下游微调     | 200           | 64–128     | 任务数据集     | 适应具体属性预测任务      |

如需进一步验证训练过程，可查阅开源代码或复现实验监控损失曲线。


1. 平均绝对误差（MAE, Mean Absolute Error）
定义：预测值与真实值绝对误差的平均值（单位：mol/kg 或 eV）。
重要性：
论文的核心评估指标，直接衡量预测精度（越小越好）。
复现目标：
QMOF（带隙预测）：MAE应接近 0.46 eV（预训练后）。
hMOF（气体吸附预测）：
CO₂吸附（0.5 bar）：MAE ≈ 3.10 mol/kg（预训练后）。
CH₄吸附（2.5 bar）：MAE ≈ 2.98 mol/kg（预训练后）。
对比基准：确保MOFormer的MAE显著低于传统方法（如Stoichiometric-120、RACs）。

# 复现
论文中的“准确率”实际以回归指标为主：代码里实现并打印的是 MAE（平均绝对误差）。在 finetune_transformer.py 与 finetune_cgcnn.py 的验证与测试阶段，都会输出并保存 MAE（见两者的 test() 方法）。
数据划分方式：由配置 valid_ratio 和 test_ratio 控制，按固定随机种子拆分（--seed 或配置里的 randomSeed）。

复现实验的最快路径是 Transformer 分支（基于 MOFid 的 CSV），因为仓库已提供预训练权重 ckpt/pretraining/model_transformer_14.pth 和数据文件 benchmark_datasets/*/mofid/*.csv。
运行入口：python finetune_transformer.py，会训练并在测试集评估 MAE，输出到 training_results/finetuning/Transformer/.../test_results.csv，并把最终 MAE 也写入一个结果 CSV。
CGCNN 分支需要图数据 .npz（配置里 root_dir: ./QMOF_cg），当前仓库未见该目录，直接复现较麻烦；若要走 CGCNN，需要先生成图数据或改回 CIFData 路径并准备 CIF。

需要调整的路径：config_ft_transformer.yaml 中 dataset.dataPath 目前是 Linux 绝对路径（/root/autodl-tmp/...），需要改成本机的相对路径（例如 benchmark_datasets/QMOF/mofid/QMOF_small_mofid.csv）。