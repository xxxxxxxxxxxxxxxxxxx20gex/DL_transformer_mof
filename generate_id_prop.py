import os
import sys
import numpy as np
import glob
import multiprocessing as mp
from multiprocessing import Pool, Manager
from tqdm import tqdm
import time
import pickle
import tempfile
import shutil

# æ·»åŠ mofidç›®å½•åˆ°Pythonè·¯å¾„ï¼Œç¡®ä¿ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç‰ˆæœ¬
mofid_path = os.path.join(os.path.dirname(__file__), 'mofid')
sys.path.insert(0, mofid_path)
from Python.run_mofid import cif2mofid

def process_single_cif(cif_file):
    try:
        # è·å–CIFæ–‡ä»¶åï¼ˆåŒ…å«.cifæ‰©å±•åï¼‰
        cif_filename = os.path.basename(cif_file)
        
        # ä½¿ç”¨MOFidç”ŸæˆMOFidå­—ç¬¦ä¸²
        mofid_result = cif2mofid(cif_file)
        mofid_string = mofid_result['mofid']
        return [cif_filename, mofid_string]
        
    except Exception as e:
        error_msg = str(e)
        # ç‰¹æ®Šå¤„ç†"More than one fragment found"é”™è¯¯
        if "More than one fragment found" in error_msg:
            # é™é»˜å¤„ç†ç‰‡æ®µé”™è¯¯ï¼Œé¿å…è¿‡å¤šè¾“å‡º
            pass
        return [os.path.basename(cif_file), None]

def process_batch_cif(args):
    """
    å¹¶è¡Œå¤„ç†ä¸€æ‰¹CIFæ–‡ä»¶
    
    Args:
        args: tuple (cif_files_batch, batch_id, temp_dir, progress_counter, progress_lock)
        
    Returns:
        tuple: (batch_id, temp_file_path, successful_count, failed_count, fragment_error_count)
    """
    cif_files_batch, batch_id, temp_dir, progress_counter, progress_lock = args
    
    # æ‰¹æ¬¡å¼€å§‹é€šçŸ¥
    print(f"ğŸš€ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_id}: {len(cif_files_batch)} ä¸ªæ–‡ä»¶")
    
    batch_results = []
    successful_count = 0
    failed_count = 0
    fragment_error_count = 0
    
    # å¤„ç†å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰æ–‡ä»¶
    for cif_file in cif_files_batch:
        result = process_single_cif(cif_file)
        batch_results.append(result)
        print(batch_results)
        if result[1] is not None:
            successful_count += 1
        else:
            failed_count += 1
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰‡æ®µé”™è¯¯
            try:
                cif2mofid(cif_file)
            except Exception as e:
                if "More than one fragment found" in str(e):
                    fragment_error_count += 1
            except:
                pass
        
        # æ›´æ–°å…¨å±€è¿›åº¦
        if progress_counter is not None and progress_lock is not None:
            with progress_lock:
                progress_counter.value += 1
    
    # ä¿å­˜æ‰¹æ¬¡ç»“æœåˆ°ä¸´æ—¶æ–‡ä»¶
    temp_file = os.path.join(temp_dir, f'batch_{batch_id}.npy')
    if batch_results:
        batch_array = np.array(batch_results, dtype=object)
        np.save(temp_file, batch_array)
    
    # æ‰¹æ¬¡å®Œæˆé€šçŸ¥
    print(f"âœ“ æ‰¹æ¬¡ {batch_id} å®Œæˆ: {len(cif_files_batch)} ä¸ªæ–‡ä»¶ "
          f"(æˆåŠŸ: {successful_count}, å¤±è´¥: {failed_count}, ç‰‡æ®µé”™è¯¯: {fragment_error_count})")
    
    return (batch_id, temp_file, successful_count, failed_count, fragment_error_count)

def generate_id_prop_npy_serial(cif_dir, output_file='id_prop.npy', save_interval=1000):
    print("æ³¨æ„: ä¸²è¡Œç‰ˆæœ¬å·²å¼ƒç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å¹¶è¡Œç‰ˆæœ¬ä»¥æå‡æ€§èƒ½")
    return generate_id_prop_npy_parallel(cif_dir, output_file, batch_size=save_interval)

def generate_id_prop_npy_parallel(cif_dir, output_file='id_prop.npy', batch_size=1000, num_processes=None):
    """
    ä¸ºCIFæ–‡ä»¶å¤¹ç”Ÿæˆid_prop.npyæ–‡ä»¶ï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼Œå†…å­˜ä¼˜åŒ–ï¼‰
    
    Args:
        cif_dir: CIFæ–‡ä»¶ç›®å½•
        output_file: è¾“å‡ºçš„npyæ–‡ä»¶å
        batch_size: æ¯æ‰¹å¤„ç†çš„æ–‡ä»¶æ•°é‡
        num_processes: è¿›ç¨‹æ•°é‡ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
    """
    cif_files = glob.glob(os.path.join(cif_dir, '*.cif'))
    print(f"æ‰¾åˆ° {len(cif_files)} ä¸ªCIFæ–‡ä»¶")
    
    if not cif_files:
        print("æœªæ‰¾åˆ°CIFæ–‡ä»¶")
        return np.array([], dtype=object)
    
    # è®¾ç½®è¿›ç¨‹æ•°é‡
    if num_processes is None:
        num_processes = min(mp.cpu_count(), 8)  # æœ€å¤š8ä¸ªè¿›ç¨‹ï¼Œé¿å…è¿‡åº¦å ç”¨èµ„æº
    
    print(f"ä½¿ç”¨ {num_processes} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp(prefix='mof_conversion_')
    print(f"ä¸´æ—¶ç›®å½•: {temp_dir}")
    
    try:
        # å°†æ–‡ä»¶åˆ†æ‰¹
        batches = []
        for i in range(0, len(cif_files), batch_size):
            batch_files = cif_files[i:i+batch_size]
            batches.append(batch_files)
        
        print(f"æ€»å…±åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡")
        
        # åˆ›å»ºå…±äº«è¿›åº¦è®¡æ•°å™¨å’Œé”
        manager = Manager()
        progress_counter = manager.Value('i', 0)
        progress_lock = manager.Lock()
        
        # å‡†å¤‡å¹¶è¡Œå¤„ç†å‚æ•°
        process_args = []
        for batch_id, batch_files in enumerate(batches):
            process_args.append((batch_files, batch_id, temp_dir, progress_counter, progress_lock))
        
        start_time = time.time()
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ‰¹æ¬¡
        with Pool(processes=num_processes) as pool:
            # å¯åŠ¨è¿›åº¦ç›‘æ§
            print("å¼€å§‹å¹¶è¡Œå¤„ç†...")
            
            # å¼‚æ­¥æ‰§è¡Œæ‰€æœ‰æ‰¹æ¬¡
            async_results = pool.map_async(process_batch_cif, process_args)
            
            # ç›‘æ§è¿›åº¦
            total_files = len(cif_files)
            with tqdm(total=total_files, desc="å¤„ç†CIFæ–‡ä»¶") as pbar:
                last_progress = 0
                while not async_results.ready():
                    current_progress = progress_counter.value
                    pbar.update(current_progress - last_progress)
                    last_progress = current_progress
                    time.sleep(1)
                
                # ç¡®ä¿è¿›åº¦æ¡å®Œæˆ
                final_progress = progress_counter.value
                pbar.update(final_progress - last_progress)
            
            # è·å–æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
            batch_results = async_results.get()
        
        # ç»Ÿè®¡å¤„ç†ç»“æœ
        total_successful = 0
        total_failed = 0
        total_fragment_errors = 0
        temp_files = []
        
        print(f"\næ‰¹æ¬¡å¤„ç†ç»“æœæ±‡æ€»:")
        print("-" * 50)
        for batch_id, temp_file, successful, failed, fragment_errors in batch_results:
            total_successful += successful
            total_failed += failed
            total_fragment_errors += fragment_errors
            if os.path.exists(temp_file):
                temp_files.append(temp_file)
            
            # æ˜¾ç¤ºæ¯ä¸ªæ‰¹æ¬¡çš„è¯¦ç»†ç»“æœ
            batch_total = successful + failed
            success_rate = (successful / batch_total * 100) if batch_total > 0 else 0
            print(f"æ‰¹æ¬¡ {batch_id:3d}: {batch_total:4d} æ–‡ä»¶, "
                  f"æˆåŠŸ {successful:4d} ({success_rate:5.1f}%), "
                  f"å¤±è´¥ {failed:3d}, ç‰‡æ®µé”™è¯¯ {fragment_errors:3d}")
        
        print(f"\næ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼")
        print(f"æˆåŠŸ: {total_successful}, å¤±è´¥: {total_failed}, ç‰‡æ®µé”™è¯¯: {total_fragment_errors}")
        
        # åˆå¹¶æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
        print("å¼€å§‹åˆå¹¶ä¸´æ—¶æ–‡ä»¶...")
        merge_temp_files(temp_files, output_file)
        
        # æœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - start_time
        print(f"\nå¤„ç†å®Œæˆï¼")
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/3600:.2f}å°æ—¶)")
        print(f"æˆåŠŸå¤„ç†: {total_successful} ä¸ªæ–‡ä»¶")
        print(f"å¤„ç†å¤±è´¥: {total_failed} ä¸ªæ–‡ä»¶")
        print(f"ç‰‡æ®µé”™è¯¯: {total_fragment_errors} ä¸ªæ–‡ä»¶")
        print(f"æˆåŠŸç‡: {total_successful/len(cif_files)*100:.1f}%")
        print(f"åŠ é€Ÿæ¯”: é¢„è®¡æ¯”ä¸²è¡Œå¤„ç†å¿« {num_processes:.1f}x")
        
        # éªŒè¯æœ€ç»ˆæ–‡ä»¶
        if os.path.exists(output_file):
            final_data = np.load(output_file, allow_pickle=True)
            print(f"æœ€ç»ˆæ–‡ä»¶åŒ…å« {len(final_data)} ä¸ªæ¡ç›®")
            return final_data
        else:
            print("æœ€ç»ˆæ–‡ä»¶ç”Ÿæˆå¤±è´¥")
            return np.array([], dtype=object)
            
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
            print(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")

def merge_temp_files(temp_files, output_file):
    """
    åˆå¹¶ä¸´æ—¶npyæ–‡ä»¶ä¸ºæœ€ç»ˆæ–‡ä»¶
    
    Args:
        temp_files: ä¸´æ—¶æ–‡ä»¶åˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    all_data = []
    
    print(f"åˆå¹¶ {len(temp_files)} ä¸ªä¸´æ—¶æ–‡ä»¶...")
    for temp_file in tqdm(temp_files, desc="åˆå¹¶æ–‡ä»¶"):
        if os.path.exists(temp_file):
            try:
                batch_data = np.load(temp_file, allow_pickle=True)
                all_data.extend(batch_data.tolist())
            except Exception as e:
                print(f"åŠ è½½ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {temp_file} - {e}")
    
    if all_data:
        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_array = np.array(all_data, dtype=object)
        np.save(output_file, final_array)
        print(f"æˆåŠŸåˆå¹¶ä¿å­˜åˆ°: {output_file}")
    else:
        print("æ²¡æœ‰æ•°æ®å¯ä»¥åˆå¹¶")

if __name__ == "__main__":
    # ä¸ºcif_all/cifæ–‡ä»¶å¤¹ç”Ÿæˆid_prop.npy
    cif_directory = "/root/autodl-tmp/MOFormer/cif_toy"  # CIFæ–‡ä»¶å¤¹è·¯å¾„
    output_file = "/root/autodl-tmp/MOFormer/cif_toy/id_prop1.npy"  # è¾“å‡ºæ–‡ä»¶è·¯å¾„

    print("=" * 60)
    print("MOFæ•°æ®é›†å¹¶è¡Œè½¬æ¢å·¥å…·")
    print("=" * 60)
    print(f"è¾“å…¥ç›®å½•: {cif_directory}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # æ£€æŸ¥ç³»ç»Ÿèµ„æº
    cpu_count = mp.cpu_count()
    print(f"ç³»ç»ŸCPUæ ¸å¿ƒæ•°: {cpu_count}")
    
    # ç”Ÿæˆid_prop.npyæ–‡ä»¶ï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼Œå†…å­˜ä¼˜åŒ–ï¼‰
    result = generate_id_prop_npy_parallel(
        cif_directory, 
        output_file,
        batch_size=500,  # æ¯æ‰¹å¤„ç†500ä¸ªæ–‡ä»¶ï¼Œå¹³è¡¡å†…å­˜å’Œæ•ˆç‡
        num_processes=min(cpu_count, 8)  # æœ€å¤šä½¿ç”¨8ä¸ªè¿›ç¨‹
    )

    print("=" * 60)
    print(f"å¤„ç†å®Œæˆï¼å…±å¤„ç†äº† {len(result)} ä¸ªCIFæ–‡ä»¶")
    print("=" * 60)
    
    # æ˜¾ç¤ºå‰5ä¸ªæ¡ç›®ä½œä¸ºéªŒè¯
    if len(result) > 0:
        print("\nå‰5ä¸ªæ¡ç›®:")
        for i, item in enumerate(result[:5]):
            print(f"{i}: {item}")
    else:
        print("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶ï¼")