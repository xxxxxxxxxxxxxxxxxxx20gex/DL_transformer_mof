#!/usr/bin/env python3
"""
CIFæ•°æ®é¢„å¤„ç†è„šæœ¬
å°†CIFæ–‡ä»¶é¢„å¤„ç†ä¸ºæ›´å¿«çš„.npzæ ¼å¼ï¼Œå¤§å¹…å‡å°‘è®­ç»ƒæ—¶çš„æ•°æ®åŠ è½½æ—¶é—´
"""

import os
import numpy as np
import torch
import pickle
from tqdm import tqdm
import argparse
from pymatgen.core.structure import Structure
from tokenizer.mof_tokenizer import MOFTokenizer
from dataset.dataset_multiview import AtomCustomJSONInitializer, GaussianDistance

def preprocess_cif_data(root_dir, output_dir, max_num_nbr=12, radius=8, dmin=0, step=0.2, 
                       vocab_path='tokenizer/vocab_full.txt', batch_size=100):
    """
    é¢„å¤„ç†CIFæ•°æ®ä¸º.npzæ ¼å¼
    
    Args:
        root_dir: CIFæ–‡ä»¶ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        max_num_nbr: æœ€å¤§é‚»å±…æ•°
        radius: æœç´¢åŠå¾„
        dmin: æœ€å°è·ç¦»
        step: æ­¥é•¿
        vocab_path: è¯æ±‡è¡¨è·¯å¾„
        batch_size: æ‰¹å¤„ç†å¤§å°
    """
    
    print(f"ğŸš€ å¼€å§‹é¢„å¤„ç†CIFæ•°æ®...")
    print(f"   è¾“å…¥ç›®å½•: {root_dir}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    id_prop_file = os.path.join(root_dir, 'id_prop.npy')
    if not os.path.exists(id_prop_file):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° {id_prop_file}")
    
    id_prop_data = np.load(id_prop_file, allow_pickle=True)
    print(f"   æ•°æ®é‡: {len(id_prop_data)} ä¸ªæ ·æœ¬")
    
    # åˆå§‹åŒ–ç»„ä»¶
    tokenizer = MOFTokenizer(vocab_path, model_max_length=512, padding_side='right')
    atom_init_file = os.path.join('benchmark_datasets/atom_init.json')
    ari = AtomCustomJSONInitializer(atom_init_file)
    gdf = GaussianDistance(dmin=dmin, dmax=radius, step=step)
    
    # é¢„å¤„ç†æ•°æ®
    processed_data = []
    
    for i, (cif_id, mofid) in enumerate(tqdm(id_prop_data, desc="é¢„å¤„ç†è¿›åº¦")):
        try:
            # è¯»å–CIFæ–‡ä»¶
            fname = cif_id if cif_id.endswith('.cif') else cif_id + '.cif'
            cif_path = os.path.join(root_dir, fname)
            
            if not os.path.exists(cif_path):
                print(f"âš ï¸  è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶: {cif_path}")
                continue
                
            crys = Structure.from_file(cif_path)
            
            # å¤„ç†token
            tokens = tokenizer.encode(mofid, max_length=512, truncation=True, padding='max_length')
            
            # å¤„ç†åŸå­ç‰¹å¾
            atom_fea = np.vstack([ari.get_atom_fea(crys[i].specie.number) 
                                 for i in range(len(crys))])
            
            # å¤„ç†é‚»å±…ç‰¹å¾
            all_nbrs = crys.get_all_neighbors(radius, include_index=True)
            all_nbrs = [sorted(nbrs, key=lambda x: x[1]) for nbrs in all_nbrs]
            
            nbr_fea_idx, nbr_fea = [], []
            for nbr in all_nbrs:
                if len(nbr) < max_num_nbr:
                    nbr_fea_idx.append(list(map(lambda x: x[2], nbr)) +
                                     [0] * (max_num_nbr - len(nbr)))
                    nbr_fea.append(list(map(lambda x: x[1], nbr)) +
                                 [radius + 1.] * (max_num_nbr - len(nbr)))
                else:
                    nbr_fea_idx.append(list(map(lambda x: x[2], nbr[:max_num_nbr])))
                    nbr_fea.append(list(map(lambda x: x[1], nbr[:max_num_nbr])))
            
            nbr_fea_idx, nbr_fea = np.array(nbr_fea_idx), np.array(nbr_fea)
            nbr_fea = gdf.expand(nbr_fea)
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            processed_data.append({
                'cif_id': cif_id,
                'mofid': mofid,
                'tokens': tokens,
                'atom_fea': atom_fea,
                'nbr_fea': nbr_fea,
                'nbr_fea_idx': nbr_fea_idx
            })
            
            # æ‰¹é‡ä¿å­˜
            if len(processed_data) >= batch_size:
                save_batch(processed_data, output_dir, i // batch_size)
                processed_data = []
                
        except Exception as e:
            print(f"âŒ å¤„ç† {cif_id} æ—¶å‡ºé”™: {e}")
            continue
    
    # ä¿å­˜å‰©ä½™æ•°æ®
    if processed_data:
        save_batch(processed_data, output_dir, len(id_prop_data) // batch_size)
    
    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        'total_samples': len(id_prop_data),
        'max_num_nbr': max_num_nbr,
        'radius': radius,
        'dmin': dmin,
        'step': step,
        'vocab_path': vocab_path
    }
    
    with open(os.path.join(output_dir, 'metadata.pkl'), 'wb') as f:
        pickle.dump(metadata, f)
    
    print(f"âœ… é¢„å¤„ç†å®Œæˆï¼æ•°æ®ä¿å­˜åœ¨: {output_dir}")

def save_batch(data_batch, output_dir, batch_idx):
    """ä¿å­˜ä¸€æ‰¹æ•°æ®"""
    batch_file = os.path.join(output_dir, f'batch_{batch_idx:04d}.npz')
    
    # æå–æ‰€æœ‰å­—æ®µ
    cif_ids = [item['cif_id'] for item in data_batch]
    mofids = [item['mofid'] for item in data_batch]
    tokens = np.array([item['tokens'] for item in data_batch])
    
    # ç”±äºatom_feaå’Œnbr_feaçš„å°ºå¯¸å¯èƒ½ä¸åŒï¼Œæˆ‘ä»¬éœ€è¦ç‰¹æ®Šå¤„ç†
    # è¿™é‡Œæˆ‘ä»¬ä¿å­˜ä¸ºåˆ—è¡¨ï¼Œåœ¨åŠ è½½æ—¶å†å¤„ç†
    atom_feas = [item['atom_fea'] for item in data_batch]
    nbr_feas = [item['nbr_fea'] for item in data_batch]
    nbr_fea_idxs = [item['nbr_fea_idx'] for item in data_batch]
    
    np.savez_compressed(
        batch_file,
        cif_ids=cif_ids,
        mofids=mofids,
        tokens=tokens,
        atom_feas=atom_feas,
        nbr_feas=nbr_feas,
        nbr_fea_idxs=nbr_fea_idxs
    )

def main():
    parser = argparse.ArgumentParser(description='é¢„å¤„ç†CIFæ•°æ®')
    parser.add_argument('--input_dir', type=str, required=True, help='CIFæ–‡ä»¶è¾“å…¥ç›®å½•')
    parser.add_argument('--output_dir', type=str, required=True, help='é¢„å¤„ç†æ•°æ®è¾“å‡ºç›®å½•')
    parser.add_argument('--max_num_nbr', type=int, default=12, help='æœ€å¤§é‚»å±…æ•°')
    parser.add_argument('--radius', type=float, default=8, help='æœç´¢åŠå¾„')
    parser.add_argument('--dmin', type=float, default=0, help='æœ€å°è·ç¦»')
    parser.add_argument('--step', type=float, default=0.2, help='æ­¥é•¿')
    parser.add_argument('--vocab_path', type=str, default='tokenizer/vocab_full.txt', help='è¯æ±‡è¡¨è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=100, help='æ‰¹å¤„ç†å¤§å°')
    
    args = parser.parse_args()
    
    preprocess_cif_data(
        root_dir=args.input_dir,
        output_dir=args.output_dir,
        max_num_nbr=args.max_num_nbr,
        radius=args.radius,
        dmin=args.dmin,
        step=args.step,
        vocab_path=args.vocab_path,
        batch_size=args.batch_size
    )

if __name__ == "__main__":
    main()
