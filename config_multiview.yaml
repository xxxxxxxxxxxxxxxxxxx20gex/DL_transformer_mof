batch_size: 128  # 4090 24G可以支持更大batch
epochs: 100
eval_every_n_epochs: 1
save_every_n_epochs: 5  # 每5个epoch保存一次检查点
fine_tune_from: None
log_every_n_steps: 50
gpu: cuda:0
vocab_path: 'tokenizer/vocab_full.txt'
cuda: True
use_amp: True  # 启用AMP混合精度训练
pin_memory: True  # 优化数据传输

optim:
  optimizer: Adam
  init_lr: 0.00001
  weight_decay: 1e-6

model_cgcnn: 
  atom_fea_len: 64
  h_fea_len: 512
  n_conv: 3
  n_h: 1

graph_dataset:
  root_dir: /media/gzl/HHDisk/lyq/cif_all/cif   # change here to use cif
  max_num_nbr: 12
  radius: 8
  dmin: 0
  step: 0.2
  random_seed: 666

dataloader:
  val_ratio: 0.05   # Please change here if you using the entire cif files, change to 0.05 as mentioned in SI of the paper
  num_workers: 16   # 大幅增加并行数据加载，解决GPU等待问题
  persistent_workers: True  # 保持worker进程，减少重启开销
  prefetch_factor: 4  # 增加预取因子，提升数据加载效率

barlow_loss:
  embed_size: 512
  lambd: 0.0051


celoss:
  epsilon: 0.5

Transformer:
  ntoken: 4021
  d_model: 512
  nhead: 8
  d_hid: 512
  nlayers: 6
  dropout: 0.1
